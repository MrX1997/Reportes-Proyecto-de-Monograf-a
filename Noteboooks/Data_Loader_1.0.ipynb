{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jairo Andres Saavedra Alfonso\n",
    "# 01 de Febrero de 2019\n",
    "# Universidad de Los Andes\n",
    "# Phycis \n",
    "######################__________________Report 01__________________######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Packages\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "import os\n",
    "\n",
    "cmd='jupyter nbconvert --to python Data_Loader_1.0.ipynb'\n",
    "os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdul = fits.open('truth_DR12Q.fits') # Open file\n",
    "info=hdul.info() # File info\n",
    "columns=hdul[1].columns # File Columns \n",
    "print(info,'/n',columns)\n",
    "data=hdul[1].data # Database of spectra with human-expert classifications \n",
    "\n",
    "\n",
    "# Reading data from data_dr12.fits. This file had the spectra from data dr12. \n",
    "hdul_2 = fits.open('data_dr12.fits') # Open file\n",
    "info=hdul_2.info() # File info \n",
    "columns=hdul_2[1].columns # File Columns \n",
    "print(hdul,'/n',columns)\n",
    "data2=hdul_2[1].data # Database of spectra\n",
    "spectra=hdul_2[0].data # Spectrum of each object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################__________________Report 02__________________######################\n",
    "\n",
    "## This week I pretend to find some correaltions betwen objects with human-expert classification and spretum from DR12\n",
    "\n",
    "# Subset of PLATE parameters of both data\n",
    "data_PLATE_1=data['PLATE']\n",
    "data_PLATE_2=data2['PLATE']\n",
    "\n",
    "# Subset of MJD parameters of both data\n",
    "data_MJD_1=data['MJD']\n",
    "data_MJD_2=data2['MJD']\n",
    "\n",
    "# Subset of FIBERID parameters of both data\n",
    "data_FIBERID_1=data['FIBERID']\n",
    "data_FIBERID_2=data2['FIBERID']\n",
    "\n",
    "# Subset of FIBERID parameters of both data\n",
    "data_ID_1=data['THING_ID']\n",
    "data_ID_2=data2['TARGETID']\n",
    "\n",
    "# I make here an intersecting set for all three parameters (PLATE, MJD, FIBERID) in both data.\n",
    "data_PLATE_CO=np.intersect1d(data_PLATE_1,data_PLATE_2)\n",
    "data_MJD_CO=np.intersect1d(data_MJD_1,data_MJD_2)\n",
    "data_FIBERID_CO=np.intersect1d(data_FIBERID_1,data_FIBERID_2)\n",
    "data_ID_CO=np.intersect1d(data_ID_1,data_ID_2)\n",
    "\n",
    "# As we can see, in both database, there is a correlation betwen the number of Plates, the modified julian day and the Fiber ID. \n",
    "print('Number of Plates use in both datasets:',data_PLATE_CO.shape)\n",
    "print('Number of MJD use in both datasets:',data_MJD_CO.shape)\n",
    "print('Number of FIBERID use in both datasets:',data_FIBERID_CO.shape)\n",
    "print('Number of FIBERID use in both datasets:',data_ID_CO.shape)\n",
    "#print(data_PLATE_1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The Spectra in this database have three main parameters: Plate ID, The modified julian day and fiber ID of the observation.\n",
    "# Let's take a look of the first spectrum.\n",
    "x=np.linspace(360,1000,443) # I cut the sample to 443 pixels in spaced log-wavelength.  \n",
    "#x=np.linspace(0,886,886)\n",
    "zero_spectrum=spectra[12030] #First spectrum\n",
    "zero_spectrum=zero_spectrum[:443]\n",
    "\n",
    "PLATE=data2['PLATE'] # Spectra's Plate ID\n",
    "MJD=data2['MJD'] # Spectra's the modified juliam day\n",
    "FIBERID=data2['FIBERID'] # Spectra's fiber ID\n",
    "\n",
    "zero_plate=PLATE[0] # zero spectrum Plate ID\n",
    "zero_mjd=MJD[0] # zero spectrum MJD\n",
    "zero_fiberid=FIBERID[0] # zero spectrum Fiber ID\n",
    "param = 'Plate = {:.2f}, mjd = {:.2f}, fiberid={:.2f}'.format(zero_plate, zero_mjd, zero_fiberid)\n",
    "plt.plot(x,np.log(zero_spectrum))\n",
    "plt.xlabel('Wavelength [nm]')\n",
    "plt.ylabel('Log-Flux [arb. units]')\n",
    "plt.title(param)\n",
    "plt.xlim([360,1000])\n",
    "plt.savefig('spectrum.png')\n",
    "plt.grid()\n",
    "\n",
    "# I noticed that each object and spectrum don't have the same label. So it's imposible to make some ID correlations.  \n",
    "print(data2['TARGETID'])\n",
    "print(data['THING_ID'])\n",
    "da=np.intersect1d(data2['TARGETID'],data['THING_ID'])\n",
    "print(da.shape)\n",
    "\n",
    "# So, in orden to make a correlation betwen identified object and spectrum we need to use all three parameters (Plate ID, MJD, FiberID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The column 'CLASS_PERSON' have a class identifier for each spectrum: STARS=1, GALAXY=4, QSO=3 and QSO_BAL=30.\n",
    "C_P=data['CLASS_PERSON'] #Class Person column \n",
    "STAR=C_P[C_P==1] # objects classified as stars\n",
    "GALAXY=C_P[C_P==4] # objects classified as galaxies \n",
    "QSO=C_P[C_P==3] # objects classified as QSO (Quasars)\n",
    "QSO_BAL=C_P[C_P==30] # objects classified as QSO BAL (Quasars with Broad Absortions Lines)\n",
    "N_C=C_P[C_P!=30]   \n",
    "N_C=N_C[N_C!=3]\n",
    "N_C=N_C[N_C!=1]\n",
    "N_C=N_C[N_C!=4] # objects wrong classified\n",
    "print('Star:',STAR.shape)\n",
    "print('Galaxy:',GALAXY.shape)\n",
    "print('QSO:',QSO.shape)\n",
    "print('QSO BAL:',QSO_BAL.shape)\n",
    "print('NN:',N_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Z_VI=data['Z_VI'] # Redshift of each object\n",
    "print(Z_VI[Z_VI==0.0].shape)\n",
    "Z_C_P=data['Z_CONF_PERSON']\n",
    "print(Z_C_P[Z_C_P==0].shape)\n",
    "T_ID=data['THING_ID']\n",
    "i=T_ID==-1\n",
    "T_ID=T_ID[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii=C_P==3\n",
    "oo=C_P==30\n",
    "ee=C_P==4\n",
    "\n",
    "Z_VI_G=Z_VI[ee]\n",
    "Z_VI_QSO=Z_VI[ii]\n",
    "Z_VI_QSO_BAL=Z_VI[oo]\n",
    "plt.hist(Z_VI_QSO,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('QSO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Z_VI_G,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('Galaxies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Z_VI_QSO_BAL,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('QSO_BAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I create two DataFrame for Superset_DR12Q and data_dr12 with only three parameters\n",
    "data={'PLATE':data_PLATE_1,'MJD':data_MJD_1,'FIBERID':data_FIBERID_1,'ID':data_ID_1}\n",
    "data=pd.DataFrame(data=data)\n",
    "\n",
    "data2={'PLATE':data_PLATE_2,'MJD':data_MJD_2,'FIBERID':data_FIBERID_2,'ID':data_ID_2}\n",
    "data2=pd.DataFrame(data=data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I convert all objects in both set to string chain in orden to combine them as one new ID.\n",
    "data['PLATE']=data['PLATE'].astype(str)\n",
    "data['MJD']=data['MJD'].astype(str)\n",
    "data['FIBERID']=data['FIBERID'].astype(str)\n",
    "#data['ID']=data['ID'].astype(str)\n",
    "\n",
    "\n",
    "data['PM'] = data['MJD'].str.cat(data['FIBERID'],sep=\"-\")\n",
    "#data['M'] = data['FIBERID'].str.cat(data['ID'],sep=\"-\")\n",
    "\n",
    "data['NEWID'] = data['PLATE'].str.cat(data['PM'],sep=\"-\")\n",
    "data_1=data.drop(columns=['PLATE','MJD','FIBERID','ID','PM']).values # New set of database 2 with new ID's\n",
    "print(data_1.dtype)\n",
    "\n",
    "data2['PLATE']=data2['PLATE'].astype(str)\n",
    "data2['MJD']=data2['MJD'].astype(str)\n",
    "data2['FIBERID']=data2['FIBERID'].astype(str)\n",
    "#data2['ID']=data2['ID'].astype(str)\n",
    "\n",
    "\n",
    "data2['PM'] = data2['MJD'].str.cat(data2['FIBERID'],sep=\"-\")\n",
    "#data2['M'] = data2['FIBERID'].str.cat(data2['ID'],sep=\"-\")\n",
    "\n",
    "data2['NEWID'] = data2['PLATE'].str.cat(data2['PM'],sep=\"-\")\n",
    "data_2=data2.drop(columns=['PLATE','MJD','FIBERID','ID','PM']).values # New set of database 2 with new ID's\n",
    "print(data_2.shape)\n",
    "p=data_2=='4869-55896-132'\n",
    "ll=data_2[p]\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With the routine of numpy intersect1d, I find the intersections elements in both sets. This elements  \n",
    "data_CO=np.array(np.intersect1d(data_1,data_2,return_indices=True))\n",
    "\n",
    "data_CO_objects=data_CO[0] # The unique new ID of each element in both sets\n",
    "data_CO_ind1=data_CO[1] # Indices of intersected elements from the original data 1 (Superset_DR12Q.fits) \n",
    "data_CO_ind2=data_CO[2] # Indices of intersected elements form the original data 2 (data_dr12.fits)\n",
    "print('I find',len(data_CO_objects),'objects with spectra from DR12')\n",
    "print(data_CO_ind1,data_CO_ind2)\n",
    "indi={'ind1':data_CO_ind1,'ind2':data_CO_ind2}\n",
    "ind=pd.DataFrame(data=indi,index=data_CO_ind1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mflux = np.ma.average(spectra[:,:443], weights=spectra[:,443:],axis=1)\n",
    "sflux = np.ma.average((spectra[:,:443]-mflux[:,None])**2, weights=flux[:,443:], axis=1)\n",
    "sflux = np.sqrt(sflux)\n",
    "spectra = (flux[:,:443]-mflux[:,None])/sflux[:,None]\n",
    "spec= pd.DataFrame(spectra)\n",
    "\n",
    "print(spec.shape)\n",
    "print(ind)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now that I know which object have a spectrum. I can make a unique database of objects\n",
    "hdul = fits.open('truth_DR12Q.fits')\n",
    "#hdul2 = fits.open('data_dr12.fits')\n",
    "data=hdul[1].data\n",
    "#info=hdul[1].columns\n",
    "\n",
    "#ti=np.array(data['THING_ID'],dtype=float)\n",
    "#pl=np.array(data['PLATE'],dtype=float)\n",
    "#mjd=np.array(data['MJD'],dtype=float)\n",
    "#fid=np.array(data['FIBERID'],dtype=float)\n",
    "cp=np.array(data['CLASS_PERSON'],dtype=float)\n",
    "z=np.array(data['Z_VI'],dtype=float)\n",
    "zc=np.array(data['Z_CONF_PERSON'],dtype=float)\n",
    "bal=np.array(data['BAL_FLAG_VI'],dtype=float)\n",
    "bi=np.array(data['BI_CIV'],dtype=float)\n",
    "\n",
    "d={'CLASS_PERSON':cp,'Z_VI':z,'Z_CONF_PERSON':zc,'BAL_FLAG_VI':bal,'BI_CIV':bi}\n",
    "data_0=pd.DataFrame(data=d)#.values #super database\n",
    "obj=data_0.loc[data_CO_ind1]\n",
    "\n",
    "print(obj.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################__________________Report 03__________________######################\n",
    "\n",
    "\n",
    "# Balance of classes \n",
    "C_P=obj['CLASS_PERSON'] #Class Person column \n",
    "STAR=C_P[C_P==1] # objects classified as stars\n",
    "GALAXY=C_P[C_P==4] # objects classified as galaxies \n",
    "QSO=C_P[C_P==3] # objects classified as QSO (Quasars)\n",
    "QSO_BAL=C_P[C_P==30] # objects classified as QSO BAL (Quasars with Broad Absortions Lines)\n",
    "N_C=C_P[C_P!=30]   \n",
    "N_C=N_C[N_C!=3]\n",
    "N_C=N_C[N_C!=1]\n",
    "N_C=N_C[N_C!=4] # objects wrong classified\n",
    "print('Stars:',STAR.shape)\n",
    "print('Galaxies:',GALAXY.shape)\n",
    "print('QSO:',QSO.shape)\n",
    "print('QSO BAL:',QSO_BAL.shape)\n",
    "print('No class:',N_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing. I remove non-classified objects also objects with negative redshift.  \n",
    "stars=obj.loc[obj['CLASS_PERSON']==1]\n",
    "galaxies=obj.loc[obj['CLASS_PERSON']==4]\n",
    "qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "qsos_bal=obj.loc[obj['CLASS_PERSON']==30]\n",
    "\n",
    "frames=[stars,galaxies,qsos,qsos_bal]\n",
    "new_obj=pd.concat(frames)#, keys=['stars', 'galaxies', 'qso','qso_bal'])\n",
    "\n",
    "#new_obj=new_obj.loc[new_obj['Z_VI']!=0]\n",
    "obj=new_obj.loc[new_obj['Z_CONF_PERSON']!=0]\n",
    "#indio=np.array(obj.index)\n",
    "#for i in range(len(indio)):\n",
    "#    print(indio[i])\n",
    "print(obj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "Z_VI_QSO=qsos.loc[qsos['Z_CONF_PERSON']==3]\n",
    "plt.hist(Z_VI_QSO,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('QSO')\n",
    "plt.savefig('histo_qso.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsos_bal=obj.loc[obj['CLASS_PERSON']==30]\n",
    "Z_VI_QSO_BAL=qsos_bal.loc[qsos_bal['Z_CONF_PERSON']==3]\n",
    "plt.hist(Z_VI_QSO_BAL,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('QSO BAL')\n",
    "plt.savefig('histo_qso_BAL.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=obj.loc[obj['CLASS_PERSON']==4]\n",
    "Z_VI_g=g.loc[g['Z_CONF_PERSON']==3]\n",
    "plt.hist(Z_VI_g,100,density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.title('Galaxies')\n",
    "plt.savefig('histo_g.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample of objects. I chosen 2500 object per class. \n",
    "stars=obj.loc[obj['CLASS_PERSON']==1]\n",
    "galaxies=obj.loc[obj['CLASS_PERSON']==4]\n",
    "qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "qsos_bal=obj.loc[obj['CLASS_PERSON']==30]\n",
    "\n",
    "N_sample=20000\n",
    "sample_star=stars.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "sample_galaxy=galaxies.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "sample_qso=qsos.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "sample_qso_bal=qsos_bal.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "\n",
    "sample_objects=pd.concat([sample_star,sample_galaxy,sample_qso,sample_qso_bal])\n",
    "\n",
    "ind_star=np.array(sample_star.index)\n",
    "ind_galaxy=np.array(sample_galaxy.index)\n",
    "ind_qso=np.array(sample_qso.index)\n",
    "ind_qso_bal=np.array(sample_qso_bal.index)\n",
    "\n",
    "indi=np.concatenate((ind_star, ind_galaxy,ind_qso,ind_qso_bal), axis=None)\n",
    "indi1=ind.loc[indi].values\n",
    "#print(indi)\n",
    "\n",
    "spectra_=np.zeros((N_sample,443))\n",
    "j=0\n",
    "for i in indi:\n",
    "    k=indi1[j,1]\n",
    "    spectra_[j,:]=np.log(abs(spectra[k,:443]))\n",
    "    j=j+1    \n",
    "spectra_=pd.DataFrame(spectra_)\n",
    "X=spectra_.replace(-np.inf,0)\n",
    "\n",
    "\n",
    "X=X.values\n",
    "\n",
    "y=sample_objects['CLASS_PERSON']\n",
    "y=y.replace([1, 4, 3, 30], [0,1,2,3]).values\n",
    "y=np.array(y,dtype=float)\n",
    "print(X.shape,y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################__________________Report 05__________________######################\n",
    "\n",
    "# my first Neural Network. SpectraNET :}\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\"\"\"\n",
    "X_train = Variable(torch.tensor([[X_train]], dtype=torch.float))\n",
    "X_test = Variable(torch.tensor([X_test], dtype=torch.float))\n",
    "y_train = Variable((torch.tensor(y_train, dtype=torch.long)))\n",
    "y_test = Variable((torch.tensor(y_test, dtype=torch.long)))\n",
    "print(X_train.shape,y_train.shape)\n",
    "#print(X_train)\n",
    "# Scaling \n",
    "X_train_max, _ = torch.max(X_train, 0)\n",
    "X_test_max, _ = torch.max(X_test, 0)\n",
    "#print(X_train_max)\n",
    "X_train = torch.div(X_train, X_train_max)\n",
    "X_test = torch.div(X_test, X_test_max)\n",
    "\n",
    "#y_train= torch.div(y_train, 100)\n",
    "#y_test= torch.div(y_train, 100)\n",
    "#y_test1=torch.tensor(X_test[0,:].reshape(1,-1),dtype=torch.float)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "batch_size=2000\n",
    "\"\"\"\n",
    "tl = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader=torch.utils.data.DataLoader(tl, batch_size=200, shuffle = True)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape,labels.shape)\n",
    "\"\"\"\n",
    "train_data = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    xt=X_train[i,:].reshape(1,-1)\n",
    "    train_data.append([Variable(torch.tensor([xt], dtype=torch.float)), y_train[i]])\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "i1, l1 = next(iter(train_loader))\n",
    "print(i1.shape,l1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    xtst=X_test[i,:].reshape(1,-1)\n",
    "    test_data.append([Variable(torch.tensor([xtst], dtype=torch.float)), y_test[i]])\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "i1, l1 = next(iter(test_loader))\n",
    "print(i1.shape,l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "epoc=10\n",
    "log_interval=10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(110, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.fc1(x))        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "\"\"\"\n",
    "# create a stochastic gradient descent optimizer\n",
    "opt = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "# create a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "LOSS=[]\n",
    "p=[]\n",
    "r=[]\n",
    "f=[]\n",
    "\n",
    "p1=[]\n",
    "r1=[]\n",
    "f1=[]\n",
    "\n",
    "p2=[]\n",
    "r2=[]\n",
    "f2=[]\n",
    "\n",
    "p3=[]\n",
    "r3=[]\n",
    "f3=[]\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "#print(dataiter.shape)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "images=images[0,0]\n",
    "print(images.shape)\n",
    "# show images\n",
    "plt.imshow(images)\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        print(inputs.shape)\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "def train(epoch):\n",
    "    #model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        print(data.shape,target.shape)\n",
    "        #images=data[batch_idx,0]\n",
    "        #plt.imshow(images)\n",
    "        #plt.show()\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "        \n",
    "for i in range(epoc):\n",
    "    train(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(epochs):\n",
    "    \n",
    "\n",
    "    opt.zero_grad()\n",
    "    data=X_train\n",
    "    net_out = net(data)\n",
    "    print(net_out)\n",
    "    #print('loss', loss.detach().item())       \n",
    "    #print(net_out.shape,y_train)\n",
    "    loss = criterion(net_out, y_train)\n",
    "    LOSS.append(loss.detach().item())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    net_o=net(X_test)\n",
    "    pred = net_o.data.max(1)[1]\n",
    "    \n",
    "    pres,recall,f_1,_=precision_recall_fscore_support(y_test, pred, average=None)\n",
    "    print(pres,recall,f_1)\n",
    "    \n",
    "    p.append(pres[0])\n",
    "    r.append(recall[0])\n",
    "    f.append(f_1[0])\n",
    "    \n",
    "    p1.append(pres[1])\n",
    "    r1.append(recall[1])\n",
    "    f1.append(f_1[1])\n",
    "    \n",
    "    p2.append(pres[2])\n",
    "    r2.append(recall[2])\n",
    "    f2.append(f_1[2])\n",
    "    \n",
    "    p3.append(pres[3])\n",
    "    r3.append(recall[3])\n",
    "    f3.append(f_1[3])\n",
    "\n",
    "plt.plot(range(epochs),np.array(LOSS),label='Train')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('NN Learning')\n",
    "plt.savefig('loss.png')\n",
    "    \n",
    "    \n",
    "test_loss=0    \n",
    "#y_pred=net(X_test)\n",
    "net_o=net(X_test)\n",
    "#test_loss += criterion(net_out, y_test).detach().\n",
    "pred = net_o.data.max(1)[1]  # get the index of the mhttp://localhost:8888/notebooks/Documents/Tesis/CNN%20qso/Reportes%20Proyecto%20de%20Monografia/Noteboooks/Weekly%20Reports.ipynb#ax log-probability\n",
    "print(pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),np.array(p),label='Stars',c='r')\n",
    "plt.plot(range(epochs),np.array(p1),label='Galaxies',c='g')\n",
    "plt.plot(range(epochs),np.array(p2),label='QSO',c='b')\n",
    "plt.plot(range(epochs),np.array(p3),label='QSO_BAL',c='silver')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "plt.savefig('precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),np.array(r),label='Stars',c='r')\n",
    "plt.plot(range(epochs),np.array(r1),label='Galaxies',c='g')\n",
    "plt.plot(range(epochs),np.array(r2),label='QSO',c='b')\n",
    "plt.plot(range(epochs),np.array(r3),label='QSO_BAL',c='silver')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "plt.savefig('recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(r),np.array(p),label='Stars',c='r')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Stars')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('prs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(r1),np.array(p1),label='Galaxies',c='g')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Galaxy Precision-Recall ')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('prg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(r2),np.array(p2),label='QSO',c='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('QSO Precision-Recall ')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('prq.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(r3),np.array(p3),label='QSO_BAL',c='silver')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('QSO_BAL Precision-Recall ')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('prqb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),np.array(f),label='Stars',c='r')\n",
    "plt.plot(range(epochs),np.array(f1),label='Galaxies',c='g')\n",
    "plt.plot(range(epochs),np.array(f2),label='QSO',c='b')\n",
    "plt.plot(range(epochs),np.array(f3),label='QSO_BAL',c='silver')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1')\n",
    "plt.title('F1')\n",
    "plt.legend()\n",
    "plt.savefig('F1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "class_names=['Star','Galaxy','QSO','QSO_BAL']\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,normalize=False,title=None,cmap=plt.cm.Blues):\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.subplots(121)\n",
    "#y_test=y_test.detach().numpy()\n",
    "#y_pred=y_pred.detach().numpy()\n",
    "#print(y_pred)\n",
    "plot_confusion_matrix(y_test, y_test, classes=class_names, title='Confusion matrix')\n",
    "plt.savefig('cm_train.png')\n",
    "#plt.subplots(122)\n",
    "plot_confusion_matrix(y_test, pred, classes=class_names, title='Confusion matrix')\n",
    "plt.savefig('cm_test.png')\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prf=precision_recall_fscore_support(y_test, pred, average=None)#,labels=['Star','Galaxy','QSO','QSO_BAL'])\n",
    "\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip = torch.randn(1, 2, 2)\n",
    "print(ip[0,0], ip[0].shape)\n",
    "#for i in enumerate(ip,0):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
