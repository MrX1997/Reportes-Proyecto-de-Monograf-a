{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jairo Andres Saavedra Alfonso\n",
    "# 01 de Febrero de 2019\n",
    "# Universidad de Los Andes\n",
    "# Phycis \n",
    "######################__________________Weekly Report__________________######################\n",
    "# Beta 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This net is brought to you by cpu\n"
     ]
    }
   ],
   "source": [
    "#Packages\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import time\n",
    "import os\n",
    "\n",
    "cmd='jupyter nbconvert --to python SpectraNET_R.ipynb'\n",
    "os.system(cmd)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print('This net is brought to you by',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epochs:800 -- Batch size:480\n"
     ]
    }
   ],
   "source": [
    "N_sample=10000\n",
    "batch_size=480\n",
    "n_iter=10000\n",
    "\n",
    "test_size=0.2 # 20%\n",
    "val_size=0.25 # 25% of trainning size\n",
    "\n",
    "n_train=int(N_sample*(1-test_size)*(1-val_size))\n",
    "epochs = int(n_iter / (n_train / batch_size))\n",
    "\n",
    "print('INFO: Epochs:{} -- Batch size:{}'.format(epochs,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "def Load_Files(file_1,file_2,N_sample,objts,classification=True):\n",
    "    hdul = fits.open(file_1) # Open file 1 -- 'truth_DR12Q.fits'\n",
    "    info=hdul.info() # File info\n",
    "    columns=hdul[1].columns # File Columns \n",
    "    print('INFO:',info,'/n',columns)\n",
    "    data=hdul[1].data # Database of spectra with human-expert classifications \n",
    "\n",
    "    # Reading data from data_dr12.fits. This file had the spectra from data dr12. \n",
    "    hdul_2 = fits.open(file_2) # Open file 2 -- 'data_dr12.fits'\n",
    "    info2=hdul_2.info() # File info \n",
    "    columns2=hdul_2[1].columns # File Columns\n",
    "    print('INFO:',info2,'/n',columns2)\n",
    "    data2=hdul_2[1].data # Database of spectra\n",
    "    spectra=hdul_2[0].data # Spectrum of each object \n",
    "    \n",
    "    # Subset of PLATE parameters of both data\n",
    "    data_PLATE_1=data['PLATE']\n",
    "    data_PLATE_2=data2['PLATE']\n",
    "\n",
    "    # Subset of MJD parameters of both data\n",
    "    data_MJD_1=data['MJD']\n",
    "    data_MJD_2=data2['MJD']\n",
    "\n",
    "    # Subset of FIBERID parameters of both data\n",
    "    data_FIBERID_1=data['FIBERID']\n",
    "    data_FIBERID_2=data2['FIBERID']\n",
    "    data_ID_1=data['THING_ID']\n",
    "    data_ID_2=data2['TARGETID']\n",
    "    \n",
    "    objts=np.asarray(objts)\n",
    "    \n",
    "    # The column 'CLASS_PERSON' have a class identifier for each spectrum: STARS=1, GALAXY=4, QSO=3 and QSO_BAL=30.\n",
    "    C_P=data['CLASS_PERSON'] #Class Person column \n",
    "    STAR=C_P[C_P==1] # objects classified as stars\n",
    "    GALAXY=C_P[C_P==4] # objects classified as galaxies \n",
    "    QSO=C_P[C_P==3] # objects classified as QSO (Quasars)\n",
    "    QSO_BAL=C_P[C_P==30] # objects classified as QSO BAL (Quasars with Broad Absortions Lines)\n",
    "    N_C=C_P[C_P!=30]   \n",
    "    N_C=N_C[N_C!=3]\n",
    "    N_C=N_C[N_C!=1]\n",
    "    N_C=N_C[N_C!=4] # objects wrong classified\n",
    "    \n",
    "    print('Star:',STAR.shape)\n",
    "    print('Galaxy:',GALAXY.shape)\n",
    "    print('QSO:',QSO.shape)\n",
    "    print('QSO BAL:',QSO_BAL.shape)\n",
    "    print('NN:',N_C.shape)\n",
    "    \n",
    "    # I create two DataFrame for Superset_DR12Q and data_dr12 with only three parameters\n",
    "    data1={'PLATE':data_PLATE_1,'MJD':data_MJD_1,'FIBERID':data_FIBERID_1,'ID':data_ID_1}\n",
    "    data1=pd.DataFrame(data=data1)\n",
    "\n",
    "    data2={'PLATE':data_PLATE_2,'MJD':data_MJD_2,'FIBERID':data_FIBERID_2,'ID':data_ID_2}\n",
    "    data2=pd.DataFrame(data=data2)\n",
    "\n",
    "    # I convert all objects in both set to string chain in orden to combine them as one new ID.\n",
    "    data1['PLATE']=data1['PLATE'].astype(str)\n",
    "    data1['MJD']=data1['MJD'].astype(str)\n",
    "    data1['FIBERID']=data1['FIBERID'].astype(str)\n",
    "    data1['PM'] = data1['MJD'].str.cat(data1['FIBERID'],sep=\"-\")\n",
    "    data1['NEWID'] = data1['PLATE'].str.cat(data1['PM'],sep=\"-\")\n",
    "    data_1=data1.drop(columns=['PLATE','MJD','FIBERID','ID','PM']).values\n",
    "\n",
    "    data2['PLATE']=data2['PLATE'].astype(str)\n",
    "    data2['MJD']=data2['MJD'].astype(str)\n",
    "    data2['FIBERID']=data2['FIBERID'].astype(str)\n",
    "    data2['PM'] = data2['MJD'].str.cat(data2['FIBERID'],sep=\"-\")\n",
    "    data2['NEWID'] = data2['PLATE'].str.cat(data2['PM'],sep=\"-\")\n",
    "    data_2=data2.drop(columns=['PLATE','MJD','FIBERID','ID','PM']).values # New set of database 2 with new ID's\n",
    "\n",
    "    # With the routine of numpy intersect1d, I find the intersections elements in both sets. This elements  \n",
    "    data_CO=np.array(np.intersect1d(data_1,data_2,return_indices=True))\n",
    "\n",
    "    data_CO_objects=data_CO[0] # The unique new ID of each element in both sets\n",
    "    data_CO_ind1=data_CO[1] # Indices of intersected elements from the original data 1 (Superset_DR12Q.fits) \n",
    "    data_CO_ind2=data_CO[2] # Indices of intersected elements form the original data 2 (data_dr12.fits)\n",
    "    print('I find',len(data_CO_objects),'objects with spectra from DR12')\n",
    "    #print(data_CO_ind1,data_CO_ind2)\n",
    "    indi={'ind1':data_CO_ind1,'ind2':data_CO_ind2}\n",
    "    ind=pd.DataFrame(data=indi,index=data_CO_ind1)\n",
    "\n",
    "    cp=np.array(data['CLASS_PERSON'],dtype=float)\n",
    "    z=np.array(data['Z_VI'],dtype=float)\n",
    "    zc=np.array(data['Z_CONF_PERSON'],dtype=float)\n",
    "    bal=np.array(data['BAL_FLAG_VI'],dtype=float)\n",
    "    bi=np.array(data['BI_CIV'],dtype=float)\n",
    "\n",
    "    d={'CLASS_PERSON':cp,'Z_VI':z,'Z_CONF_PERSON':zc,'BAL_FLAG_VI':bal,'BI_CIV':bi}\n",
    "    data_0=pd.DataFrame(data=d)#.values #super database\n",
    "    \n",
    "    obj=data_0.loc[data_CO_ind1]\n",
    "    \n",
    "    if(classification!=True):\n",
    "\n",
    "        if(objts[0]=='QSO'):\n",
    "            qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "            sample_objects=qsos.sample(n=int(N_sample),weights='CLASS_PERSON', random_state=5)\n",
    "            \n",
    "            indi=np.array(sample_objects.index)\n",
    "            indi1=ind.loc[indi].values\n",
    "            \n",
    "        elif(objts[0]=='QSO_BAL'):\n",
    "            qsos_bal=obj.loc[obj['CLASS_PERSON']==30]\n",
    "            sample_objects=qsos_bal.sample(n=int(N_sample),weights='CLASS_PERSON', random_state=5)\n",
    "            \n",
    "            indi=np.array(sample_objects.index)\n",
    "            indi1=ind.loc[indi].values\n",
    "            \n",
    "        elif(len(objts)==2):\n",
    "            qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "            qsos_bal=obj.loc[obj['CLASS_PERSON']==30]    \n",
    "            \n",
    "            sample_qso=qsos.sample(n=int(N_sample/2),weights='CLASS_PERSON', random_state=5)\n",
    "            sample_qso_bal=qsos_bal.sample(n=int(N_sample/2),weights='CLASS_PERSON', random_state=5)\n",
    "            sample_objects=pd.concat([sample_qso,sample_qso_bal])\n",
    "            \n",
    "            ind_qso=np.array(sample_qso.index)\n",
    "            ind_qso_bal=np.array(sample_qso_bal.index)\n",
    "            \n",
    "            indi=np.concatenate((ind_qso,ind_qso_bal), axis=None)\n",
    "            indi1=ind.loc[indi].values\n",
    "\n",
    "        spectra_=np.zeros((N_sample,886))\n",
    "            \n",
    "        j=0\n",
    "        for i in indi:\n",
    "            k=indi1[j,1]\n",
    "            spectra_[j,:]=spectra[k,:]\n",
    "            j=j+1    \n",
    "            \n",
    "        spectra_=pd.DataFrame(spectra_)\n",
    "        X=spectra_.values\n",
    "        \n",
    "        mean_flx= np.ma.average(X[:,:443], weights=X[:,443:],axis=1)\n",
    "        ll=(X[:,:443]-mean_flx.reshape(-1,1))**2\n",
    "        aveflux=np.ma.average(ll, weights=X[:,443:],axis=1)\n",
    "        sflux = np.sqrt(aveflux)\n",
    "        X = (X[:,:443]-mean_flx.reshape(-1,1))/sflux.reshape(-1,1)        \n",
    "        \n",
    "        y=sample_objects['Z_VI']\n",
    "        #y_max=torch.max(y)\n",
    "        #y=y/y_max\n",
    "        y=np.array(y,dtype=float)\n",
    "        y_max=np.max(y)\n",
    "        y=y/y_max\n",
    "        print(y)\n",
    "        return X,y\n",
    "            \n",
    "    stars=obj.loc[obj['CLASS_PERSON']==1]\n",
    "    galaxies=obj.loc[obj['CLASS_PERSON']==4]\n",
    "    qsos=obj.loc[obj['CLASS_PERSON']==3]\n",
    "    qsos_bal=obj.loc[obj['CLASS_PERSON']==30]\n",
    "\n",
    "    sample_star=stars.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "    sample_galaxy=galaxies.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "    sample_qso=qsos.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "    sample_qso_bal=qsos_bal.sample(n=int(N_sample/4),weights='CLASS_PERSON', random_state=5)\n",
    "\n",
    "    sample_objects=pd.concat([sample_star,sample_galaxy,sample_qso,sample_qso_bal])\n",
    "\n",
    "    ind_star=np.array(sample_star.index)\n",
    "    ind_galaxy=np.array(sample_galaxy.index)\n",
    "    ind_qso=np.array(sample_qso.index)\n",
    "    ind_qso_bal=np.array(sample_qso_bal.index)\n",
    "\n",
    "    indi=np.concatenate((ind_star, ind_galaxy,ind_qso,ind_qso_bal), axis=None)\n",
    "    indi1=ind.loc[indi].values\n",
    "\n",
    "    spectra_=np.zeros((N_sample,886))\n",
    "    j=0\n",
    "    for i in indi:\n",
    "        k=indi1[j,1]\n",
    "        spectra_[j,:]=spectra[k,:]#np.log(abs(spectra[k,:443]))\n",
    "        j=j+1    \n",
    "        \n",
    "        \n",
    "    spectra_=pd.DataFrame(spectra_)\n",
    "    X=spectra_.values\n",
    "    \n",
    "    #Renormalize spectra\n",
    "    \n",
    "    mean_flx= np.ma.average(X[:,:443], weights=X[:,443:],axis=1)\n",
    "    ll=(X[:,:443]-mean_flx.reshape(-1,1))**2\n",
    "    aveflux=np.ma.average(ll, weights=X[:,443:],axis=1)\n",
    "    sflux = np.sqrt(aveflux)\n",
    "    X = (X[:,:443]-mean_flx.reshape(-1,1))/sflux.reshape(-1,1)\n",
    "\n",
    "    y=sample_objects['CLASS_PERSON']\n",
    "    y=y.replace([1, 4, 3, 30], [0,1,2,3]).values\n",
    "    y=np.array(y,dtype=float)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def Loader(X,y,N_sample):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for i in range(y_train.shape[0]):\n",
    "        xt=X_train[i,:].reshape(1,-1)\n",
    "        train_data.append([Variable(torch.tensor(xt, dtype=torch.float)), torch.tensor(y_train[i], dtype=torch.float)])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    test_data = []\n",
    "    for i in range(y_test.shape[0]):\n",
    "        xtst=X_test[i,:].reshape(1,-1)\n",
    "        test_data.append([Variable(torch.tensor(xtst, dtype=torch.float)), torch.tensor(y_test[i], dtype=torch.float)])\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    val_data = []\n",
    "    for i in range(y_val.shape[0]):\n",
    "        xv=X_val[i,:].reshape(1,-1)\n",
    "        val_data.append([Variable(torch.tensor(xv, dtype=torch.float)), torch.tensor(y_val[i], dtype=torch.float)])\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    print('INFO')\n",
    "    print('Train shape:',y_train.shape[0])\n",
    "    print('Val shape:',y_val.shape[0])\n",
    "    print('Test shape:',y_test.shape[0])\n",
    "    return train_loader,test_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: truth_DR12Q.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU       6   ()      \n",
      "  1                1 BinTableHDU     27   546856R x 9C   [J, D, J, J, J, J, J, D, D]   \n",
      "INFO: None /n ColDefs(\n",
      "    name = 'THING_ID'; format = 'J'\n",
      "    name = 'Z_VI'; format = 'D'\n",
      "    name = 'PLATE'; format = 'J'\n",
      "    name = 'MJD'; format = 'J'\n",
      "    name = 'FIBERID'; format = 'J'\n",
      "    name = 'CLASS_PERSON'; format = 'J'\n",
      "    name = 'Z_CONF_PERSON'; format = 'J'\n",
      "    name = 'BAL_FLAG_VI'; format = 'D'\n",
      "    name = 'BI_CIV'; format = 'D'\n",
      ")\n",
      "Filename: data_dr12.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU       8   (886, 639464)   float64   \n",
      "  1                1 BinTableHDU     16   639464R x 4C   [J, J, J, J]   \n",
      "INFO: None /n ColDefs(\n",
      "    name = 'TARGETID'; format = 'J'\n",
      "    name = 'PLATE'; format = 'J'\n",
      "    name = 'MJD'; format = 'J'\n",
      "    name = 'FIBERID'; format = 'J'\n",
      ")\n",
      "Star: (207915,)\n",
      "Galaxy: (22795,)\n",
      "QSO: (270686,)\n",
      "QSO BAL: (29659,)\n",
      "NN: (15801,)\n",
      "I find 537677 objects with spectra from DR12\n",
      "[0.45076142 0.50558376 0.45989848 ... 0.47715736 0.60791878 0.44994924]\n",
      "INFO\n",
      "Train shape: 6000\n",
      "Val shape: 2000\n",
      "Test shape: 2000\n"
     ]
    }
   ],
   "source": [
    "X,y=Load_Files('truth_DR12Q.fits','data_dr12.fits',N_sample,['QSO'],classification=False)\n",
    "train_loader,test_loader,val_loader=Loader(X,y,N_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "SpectraNET",
     "Regression"
    ]
   },
   "outputs": [],
   "source": [
    "class Net_R(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_R, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 10,stride=2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 10,stride=2)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 10,stride=2)\n",
    "        self.conv4 = nn.Conv1d(256, 256, 10,stride=2)\n",
    "        self.pool = nn.MaxPool1d(2, 1)\n",
    "        self.fc1 = nn.Linear(4608, 128)\n",
    "        self.bn=nn.BatchNorm1d(128)\n",
    "        #self.fc1 = nn.Linear(10300, 16)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        ##x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))     \n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_R(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(10,), stride=(2,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(10,), stride=(2,))\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(10,), stride=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(10,), stride=(2,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Epoc: 1\n",
      "Batch: 1 <--> Loss: 0.6860418319702148\n",
      "Batch: 2 <--> Loss: 0.4229004979133606\n",
      "Batch: 3 <--> Loss: 0.23446185886859894\n",
      "Batch: 4 <--> Loss: 0.18533842265605927\n",
      "Batch: 5 <--> Loss: 0.16593943536281586\n",
      "Batch: 6 <--> Loss: 0.16003678739070892\n",
      "Batch: 7 <--> Loss: 0.16509999334812164\n",
      "Batch: 8 <--> Loss: 0.15018686652183533\n",
      "Batch: 9 <--> Loss: 0.14357836544513702\n",
      "Batch: 10 <--> Loss: 0.14677436649799347\n",
      "Batch: 11 <--> Loss: 0.1436922252178192\n",
      "Batch: 12 <--> Loss: 0.14399966597557068\n",
      "Batch: 13 <--> Loss: 0.14076949656009674\n",
      "Epoc: 2\n",
      "Batch: 1 <--> Loss: 0.1414329707622528\n",
      "Batch: 2 <--> Loss: 0.13728207349777222\n",
      "Batch: 3 <--> Loss: 0.14092037081718445\n",
      "Batch: 4 <--> Loss: 0.13757051527500153\n",
      "Batch: 5 <--> Loss: 0.129802867770195\n",
      "Batch: 6 <--> Loss: 0.13362064957618713\n",
      "Batch: 7 <--> Loss: 0.1352454125881195\n",
      "Batch: 8 <--> Loss: 0.1427801549434662\n",
      "Batch: 9 <--> Loss: 0.12932106852531433\n",
      "Batch: 10 <--> Loss: 0.13140927255153656\n",
      "Batch: 11 <--> Loss: 0.12887896597385406\n",
      "Batch: 12 <--> Loss: 0.12680518627166748\n",
      "Batch: 13 <--> Loss: 0.13485407829284668\n",
      "Epoc: 3\n",
      "Batch: 1 <--> Loss: 0.12943296134471893\n",
      "Batch: 2 <--> Loss: 0.12974755465984344\n",
      "Batch: 3 <--> Loss: 0.13028684258460999\n",
      "Batch: 4 <--> Loss: 0.14443203806877136\n",
      "Batch: 5 <--> Loss: 0.1314568966627121\n",
      "Batch: 6 <--> Loss: 0.12605488300323486\n",
      "Batch: 7 <--> Loss: 0.13619571924209595\n",
      "Batch: 8 <--> Loss: 0.1248372346162796\n",
      "Batch: 9 <--> Loss: 0.13250797986984253\n",
      "Batch: 10 <--> Loss: 0.1252262145280838\n",
      "Batch: 11 <--> Loss: 0.13620340824127197\n",
      "Batch: 12 <--> Loss: 0.12535858154296875\n",
      "Batch: 13 <--> Loss: 0.12967443466186523\n",
      "Epoc: 4\n",
      "Batch: 1 <--> Loss: 0.1295035481452942\n",
      "Batch: 2 <--> Loss: 0.13521338999271393\n",
      "Batch: 3 <--> Loss: 0.13601580262184143\n",
      "Batch: 4 <--> Loss: 0.12138762325048447\n",
      "Batch: 5 <--> Loss: 0.1235792338848114\n",
      "Batch: 6 <--> Loss: 0.1254674345254898\n",
      "Batch: 7 <--> Loss: 0.13001374900341034\n",
      "Batch: 8 <--> Loss: 0.1376432478427887\n",
      "Batch: 9 <--> Loss: 0.13373546302318573\n",
      "Batch: 10 <--> Loss: 0.12741151452064514\n",
      "Batch: 11 <--> Loss: 0.12970124185085297\n",
      "Batch: 12 <--> Loss: 0.13577033579349518\n",
      "Batch: 13 <--> Loss: 0.12156215310096741\n",
      "Epoc: 5\n",
      "Batch: 1 <--> Loss: 0.1295897215604782\n",
      "Batch: 2 <--> Loss: 0.12781909108161926\n",
      "Batch: 3 <--> Loss: 0.13308925926685333\n",
      "Batch: 4 <--> Loss: 0.13016587495803833\n",
      "Batch: 5 <--> Loss: 0.13025736808776855\n",
      "Batch: 6 <--> Loss: 0.1290363073348999\n",
      "Batch: 7 <--> Loss: 0.13439662754535675\n",
      "Batch: 8 <--> Loss: 0.12421055138111115\n",
      "Batch: 9 <--> Loss: 0.1351759135723114\n",
      "Batch: 10 <--> Loss: 0.12354255467653275\n",
      "Batch: 11 <--> Loss: 0.12643909454345703\n",
      "Batch: 12 <--> Loss: 0.13525494933128357\n",
      "Batch: 13 <--> Loss: 0.12664107978343964\n",
      "Epoc: 6\n",
      "Batch: 1 <--> Loss: 0.1288660615682602\n",
      "Batch: 2 <--> Loss: 0.12319657951593399\n",
      "Batch: 3 <--> Loss: 0.12444892525672913\n",
      "Batch: 4 <--> Loss: 0.13420021533966064\n",
      "Batch: 5 <--> Loss: 0.13577431440353394\n",
      "Batch: 6 <--> Loss: 0.12692037224769592\n",
      "Batch: 7 <--> Loss: 0.12979091703891754\n",
      "Batch: 8 <--> Loss: 0.13636726140975952\n",
      "Batch: 9 <--> Loss: 0.13164323568344116\n",
      "Batch: 10 <--> Loss: 0.13263371586799622\n",
      "Batch: 11 <--> Loss: 0.12515895068645477\n",
      "Batch: 12 <--> Loss: 0.12726007401943207\n",
      "Batch: 13 <--> Loss: 0.13566496968269348\n",
      "Epoc: 7\n",
      "Batch: 1 <--> Loss: 0.13234180212020874\n",
      "Batch: 2 <--> Loss: 0.13146285712718964\n",
      "Batch: 3 <--> Loss: 0.14027656614780426\n",
      "Batch: 4 <--> Loss: 0.1354941725730896\n",
      "Batch: 5 <--> Loss: 0.12615226209163666\n",
      "Batch: 6 <--> Loss: 0.12984415888786316\n",
      "Batch: 7 <--> Loss: 0.13582031428813934\n",
      "Batch: 8 <--> Loss: 0.13267628848552704\n",
      "Batch: 9 <--> Loss: 0.12404140084981918\n",
      "Batch: 10 <--> Loss: 0.1352936178445816\n",
      "Batch: 11 <--> Loss: 0.12937156856060028\n",
      "Batch: 12 <--> Loss: 0.12749989330768585\n",
      "Batch: 13 <--> Loss: 0.12105417251586914\n",
      "Epoc: 8\n",
      "Batch: 1 <--> Loss: 0.1243688315153122\n",
      "Batch: 2 <--> Loss: 0.1230369508266449\n",
      "Batch: 3 <--> Loss: 0.13463318347930908\n",
      "Batch: 4 <--> Loss: 0.1273307204246521\n",
      "Batch: 5 <--> Loss: 0.1285111904144287\n",
      "Batch: 6 <--> Loss: 0.12200705707073212\n",
      "Batch: 7 <--> Loss: 0.13750649988651276\n",
      "Batch: 8 <--> Loss: 0.1348150074481964\n",
      "Batch: 9 <--> Loss: 0.12461932748556137\n",
      "Batch: 10 <--> Loss: 0.13302892446517944\n",
      "Batch: 11 <--> Loss: 0.1325671523809433\n",
      "Batch: 12 <--> Loss: 0.12337961792945862\n",
      "Batch: 13 <--> Loss: 0.13278935849666595\n",
      "Epoc: 9\n",
      "Batch: 1 <--> Loss: 0.12527647614479065\n",
      "Batch: 2 <--> Loss: 0.13232213258743286\n",
      "Batch: 3 <--> Loss: 0.12453682720661163\n",
      "Batch: 4 <--> Loss: 0.13858923316001892\n",
      "Batch: 5 <--> Loss: 0.1375533938407898\n",
      "Batch: 6 <--> Loss: 0.12162476778030396\n",
      "Batch: 7 <--> Loss: 0.135684534907341\n",
      "Batch: 8 <--> Loss: 0.13449794054031372\n",
      "Batch: 9 <--> Loss: 0.12652963399887085\n",
      "Batch: 10 <--> Loss: 0.1337158977985382\n",
      "Batch: 11 <--> Loss: 0.1276317536830902\n",
      "Batch: 12 <--> Loss: 0.12418870627880096\n",
      "Batch: 13 <--> Loss: 0.12599466741085052\n",
      "Epoc: 10\n",
      "Batch: 1 <--> Loss: 0.135026216506958\n",
      "Batch: 2 <--> Loss: 0.13081969320774078\n",
      "Batch: 3 <--> Loss: 0.13166522979736328\n",
      "Batch: 4 <--> Loss: 0.1337321400642395\n",
      "Batch: 5 <--> Loss: 0.12536223232746124\n",
      "Batch: 6 <--> Loss: 0.13124701380729675\n",
      "Batch: 7 <--> Loss: 0.12547841668128967\n",
      "Batch: 8 <--> Loss: 0.13817176222801208\n",
      "Batch: 9 <--> Loss: 0.1334734410047531\n",
      "Batch: 10 <--> Loss: 0.13353219628334045\n",
      "Batch: 11 <--> Loss: 0.12206564843654633\n",
      "Batch: 12 <--> Loss: 0.12627969682216644\n",
      "Batch: 13 <--> Loss: 0.13106763362884521\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "learning_rate=0.01\n",
    "log_interval=10\n",
    "epoc=10\n",
    "\n",
    "net_R = Net_R()\n",
    "print(net_R)\n",
    "\n",
    "optimizer = torch.optim.SGD(net_R.parameters(), lr=0.2) #for Rgrss\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "loss_=[]\n",
    "def train(epoch):\n",
    "    #model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader,0):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net_R(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss = loss.item()\n",
    "        loss_.append(running_loss)\n",
    "        print('Batch:',batch_idx+1,'<-->','Loss:',running_loss)\n",
    "        #if(batch_idx !=0):    # print every 2000 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 1000))\n",
    "            #running_loss = 0.0\n",
    "    \n",
    "\n",
    "\n",
    "# Training loop\n",
    "for i in range(epoc):\n",
    "    print('Epoc:',i+1)\n",
    "    train(i)\n",
    "    \n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "loss_=np.asarray(loss_)\n",
    "\n",
    "epoch=np.linspace(0,len(loss_),len(loss_))\n",
    "\n",
    "plt.plot(epoch,loss_,label='Training loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss - Regression')\n",
    "plt.legend()\n",
    "plt.savefig('Train_loss_Regression.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5442, 0.2538, 0.4924, 0.5919, 0.4250, 0.4761, 0.4473, 0.3281, 0.4341,\n",
      "        0.4705, 0.5196, 0.3423, 0.4678, 0.4705, 0.5155, 0.4812, 0.1456, 0.1547,\n",
      "        0.5287, 0.3819, 0.4530, 0.3762, 0.3675, 0.4294, 0.2420, 0.4983, 0.4853,\n",
      "        0.4238, 0.5356, 0.4948, 0.4132, 0.5322, 0.5202, 0.2024, 0.1393, 0.4217,\n",
      "        0.1578, 0.1257, 0.1773, 0.2102, 0.5458, 0.4711, 0.3988, 0.4800, 0.4701,\n",
      "        0.5460, 0.4747, 0.1909, 0.4719, 0.4995, 0.5626, 0.3513, 0.5170, 0.4644,\n",
      "        0.4887, 0.7553, 0.1647, 0.3799, 0.3468, 0.5803, 0.4540, 0.5220, 0.1791,\n",
      "        0.4508, 0.5543, 0.6640, 0.1200, 0.7842, 0.4725, 0.6008, 0.6439, 0.3403,\n",
      "        0.5655, 0.4597, 0.2698, 0.1925, 0.4211, 0.6327, 0.1498, 0.1653, 0.6126,\n",
      "        0.4747, 0.2532, 0.1695, 0.4617, 0.4711, 0.5283, 0.5375, 0.4491, 0.7736,\n",
      "        0.1259, 0.2755, 0.1387, 0.4528, 0.5614, 0.5007, 0.1525, 0.4918, 0.4902,\n",
      "        0.1555, 0.5444, 0.1364, 0.4820, 0.4004, 0.2053, 0.5817, 0.5251, 0.4469,\n",
      "        0.4956, 0.4288, 0.5216, 0.4591, 0.4365, 0.4855, 0.5553, 0.4408, 0.2587,\n",
      "        0.6883, 0.2981, 0.4999, 0.4820, 0.1726, 0.2705, 0.4542, 0.1204, 0.1710,\n",
      "        0.4370, 0.4110, 0.4231, 0.1665, 0.2916, 0.5232, 0.5651, 0.5212, 0.5066,\n",
      "        0.4599, 0.4790, 0.4960, 0.5507, 0.6061, 0.5381, 0.4345, 0.4432, 0.4097,\n",
      "        0.3267, 0.6075, 0.5870, 0.2562, 0.6400, 0.4826, 0.6154, 0.4751, 0.4719,\n",
      "        0.4761, 0.4765, 0.4128, 0.7042, 0.4670, 0.4303, 0.1584, 0.1726, 0.1758,\n",
      "        0.7048, 0.5101, 0.5163, 0.6268, 0.4902, 0.5086, 0.1722, 0.3829, 0.6026,\n",
      "        0.1545, 0.5403, 0.5285, 0.8591, 0.6032, 0.6845, 0.7953, 0.5468, 0.1974,\n",
      "        0.0804, 0.4593, 0.5702, 0.4938, 0.5756, 0.2037, 0.5048, 0.7959, 0.4597,\n",
      "        0.5344, 0.1671, 0.2631, 0.4899, 0.4566, 0.4355, 0.3470, 0.4849, 0.1377,\n",
      "        0.1330, 0.2966, 0.5590, 0.3789, 0.1052, 0.5456, 0.3890, 0.1287, 0.4946,\n",
      "        0.6749, 0.2018, 0.3196, 0.3417, 0.2619, 0.3393, 0.5762, 0.4607, 0.4329,\n",
      "        0.4351, 0.4985, 0.4962, 0.1706, 0.5665, 0.6443, 0.4652, 0.5210, 0.8010,\n",
      "        0.6321, 0.4859, 0.2471, 0.4688, 0.5165, 0.5628, 0.4554, 0.3492, 0.4298,\n",
      "        0.5466, 0.4609, 0.4975, 0.5295, 0.5200, 0.4770, 0.1563, 0.1054, 0.4747,\n",
      "        0.1750, 0.4668, 0.4378, 0.1480, 0.5996, 0.4424, 0.1829, 0.4481, 0.4432,\n",
      "        0.0934, 0.4936, 0.2904, 0.1854, 0.3697, 0.5295, 0.5821, 0.1474, 0.2908,\n",
      "        0.4863, 0.5208, 0.3868, 0.1395, 0.5046, 0.2132, 0.1476, 0.2469, 0.1503,\n",
      "        0.4682, 0.5001, 0.1655, 0.4603, 0.6252, 0.4473, 0.3553, 0.6026, 0.4958,\n",
      "        0.3314, 0.1007, 0.2089, 0.6327, 0.3174, 0.5901, 0.1568, 0.1746, 0.4396,\n",
      "        0.6483, 0.7515, 0.1003, 0.4701, 0.6118, 0.6796, 0.2301, 0.4248, 0.3568,\n",
      "        0.1405, 0.5413, 0.4861, 0.6274, 0.3218, 0.6032, 0.3488, 0.1399, 0.5584,\n",
      "        0.4467, 0.3001, 0.4164, 0.1519, 0.5732, 0.5836, 0.7417, 0.5304, 0.1245,\n",
      "        0.4242, 0.2140, 0.6156, 0.4211, 0.4631, 0.4581, 0.6368, 0.3959, 0.5894,\n",
      "        0.4731, 0.4140, 0.4975, 0.4097, 0.3101, 0.5214, 0.4741, 0.4114, 0.6432,\n",
      "        0.4629, 0.6491, 0.3052, 0.1990, 0.6242, 0.5653, 0.3009, 0.5614, 0.3395,\n",
      "        0.1064, 0.5088, 0.2646, 0.1671, 0.1462, 0.4859, 0.1474, 0.5503, 0.5170,\n",
      "        0.4386, 0.4709, 0.7787, 0.5990, 0.1907, 0.5232, 0.4625, 0.3202, 0.4376,\n",
      "        0.1346, 0.6629, 0.6209, 0.5590, 0.2804, 0.1760, 0.5691, 0.1289, 0.6341,\n",
      "        0.4749, 0.6032, 0.5468, 0.1543, 0.6430, 0.4337, 0.5149, 0.3959, 0.2422,\n",
      "        0.4512, 0.5001, 0.3533, 0.7072, 0.5395, 0.5184, 0.4130, 0.2250, 0.4577,\n",
      "        0.6016, 0.4942, 0.4701, 0.6664, 0.2264, 0.1675, 0.6621, 0.4579, 0.4638,\n",
      "        0.4575, 0.1594, 0.1237, 0.4946, 0.5606, 0.3931, 0.6835, 0.3722, 0.4491,\n",
      "        0.1316, 0.4546, 0.5007, 0.4778, 0.1190, 0.2605, 0.4041, 0.4824, 0.4502,\n",
      "        0.3582, 0.4617, 0.4581, 0.5001, 0.4863, 0.1015, 0.4625, 0.1925, 0.1123,\n",
      "        0.6906, 0.4664, 0.5549, 0.5959, 0.4037, 0.4881, 0.5409, 0.1436, 0.4751,\n",
      "        0.5474, 0.2095, 0.2104, 0.5066, 0.2991, 0.2818, 0.4416, 0.5168, 0.3537,\n",
      "        0.2652, 0.3659, 0.5230, 0.5436, 0.4447, 0.4193, 0.4739, 0.5466, 0.3184,\n",
      "        0.4634, 0.2258, 0.3431, 0.4944, 0.3322, 0.4512, 0.0841, 0.6808, 0.1901,\n",
      "        0.2520, 0.2747, 0.4079, 0.3689, 0.4420, 0.4916, 0.2709, 0.6747, 0.4495,\n",
      "        0.4617, 0.3048, 0.5038, 0.5001, 0.1884, 0.5391, 0.2491, 0.4607, 0.5561,\n",
      "        0.6368, 0.1596, 0.5858])\n",
      "tensor([0.4647, 0.4645, 0.4654, 0.4644, 0.4645, 0.4646, 0.4537, 0.4647, 0.4649,\n",
      "        0.4640, 0.4644, 0.4643, 0.4639, 0.4643, 0.4648, 0.4666, 0.4648, 0.4655,\n",
      "        0.4647, 0.4640, 0.4631, 0.4696, 0.4639, 0.4653, 0.4652, 0.4642, 0.4639,\n",
      "        0.4648, 0.4666, 0.4652, 0.4612, 0.4647, 0.4502, 0.4646, 0.4649, 0.4659,\n",
      "        0.4614, 0.4656, 0.4648, 0.4629, 0.4648, 0.4646, 0.4643, 0.4652, 0.4651,\n",
      "        0.4659, 0.4631, 0.4685, 0.4643, 0.4655, 0.4651, 0.4650, 0.4686, 0.4475,\n",
      "        0.4645, 0.4666, 0.4644, 0.4645, 0.4650, 0.4641, 0.4649, 0.4647, 0.4648,\n",
      "        0.4643, 0.4728, 0.4643, 0.4657, 0.4641, 0.4659, 0.4645, 0.4657, 0.4698,\n",
      "        0.4656, 0.4651, 0.4677, 0.4671, 0.4777, 0.4661, 0.4645, 0.4649, 0.4404,\n",
      "        0.4635, 0.4683, 0.4538, 0.4653, 0.4721, 0.4656, 0.4646, 0.4648, 0.4649,\n",
      "        0.4594, 0.4754, 0.4651, 0.4584, 0.4649, 0.4647, 0.4669, 0.4645, 0.4634,\n",
      "        0.4665, 0.4645, 0.4646, 0.4672, 0.4612, 0.4645, 0.4751, 0.4650, 0.4654,\n",
      "        0.4647, 0.4652, 0.4647, 0.4646, 0.4645, 0.4646, 0.4647, 0.4640, 0.4708,\n",
      "        0.4639, 0.4655, 0.4656, 0.4637, 0.4660, 0.4647, 0.4592, 0.4645, 0.4638,\n",
      "        0.4652, 0.4704, 0.4649, 0.4653, 0.4645, 0.4656, 0.4720, 0.4647, 0.4656,\n",
      "        0.4705, 0.4628, 0.4649, 0.4647, 0.4640, 0.4661, 0.4648, 0.4518, 0.4671,\n",
      "        0.4627, 0.4630, 0.4655, 0.4650, 0.4645, 0.4646, 0.4692, 0.4652, 0.4644,\n",
      "        0.4634, 0.4650, 0.4647, 0.4565, 0.4649, 0.4640, 0.4645, 0.4641, 0.4655,\n",
      "        0.4657, 0.4715, 0.4353, 0.4589, 0.4660, 0.4652, 0.4646, 0.4646, 0.4634,\n",
      "        0.4632, 0.4648, 0.4590, 0.4613, 0.4648, 0.4280, 0.4737, 0.4632, 0.4631,\n",
      "        0.4685, 0.4641, 0.4646, 0.4649, 0.4672, 0.4659, 0.4646, 0.4680, 0.4646,\n",
      "        0.4649, 0.4648, 0.4651, 0.4651, 0.4650, 0.4663, 0.4627, 0.4645, 0.4666,\n",
      "        0.4633, 0.4648, 0.4646, 0.4641, 0.4641, 0.4655, 0.4648, 0.4646, 0.4648,\n",
      "        0.4644, 0.4652, 0.4635, 0.4647, 0.4645, 0.4645, 0.4666, 0.4662, 0.4654,\n",
      "        0.4657, 0.4457, 0.4663, 0.4643, 0.4671, 0.4650, 0.4640, 0.4601, 0.4647,\n",
      "        0.4646, 0.4651, 0.4658, 0.4671, 0.4757, 0.4650, 0.4638, 0.4645, 0.4645,\n",
      "        0.4640, 0.4649, 0.4660, 0.4693, 0.4646, 0.4661, 0.4635, 0.4643, 0.4645,\n",
      "        0.4614, 0.4644, 0.4464, 0.4647, 0.4654, 0.4471, 0.4610, 0.4663, 0.4646,\n",
      "        0.4584, 0.4605, 0.4664, 0.4730, 0.4655, 0.4644, 0.4662, 0.4646, 0.4674,\n",
      "        0.4637, 0.4653, 0.4655, 0.4653, 0.4661, 0.4646, 0.4655, 0.4651, 0.4663,\n",
      "        0.4644, 0.4630, 0.4648, 0.4640, 0.4685, 0.4642, 0.4649, 0.4645, 0.4788,\n",
      "        0.4635, 0.4655, 0.4651, 0.4646, 0.4646, 0.4745, 0.4645, 0.4643, 0.4655,\n",
      "        0.4646, 0.4655, 0.4644, 0.4646, 0.4650, 0.4661, 0.4650, 0.4646, 0.4648,\n",
      "        0.4623, 0.4649, 0.4651, 0.4613, 0.4294, 0.4655, 0.4661, 0.4622, 0.4557,\n",
      "        0.4655, 0.4624, 0.4634, 0.4708, 0.4695, 0.4649, 0.4646, 0.4650, 0.4655,\n",
      "        0.4647, 0.4594, 0.4566, 0.4660, 0.4646, 0.4647, 0.4642, 0.4642, 0.4650,\n",
      "        0.4643, 0.4695, 0.4641, 0.4549, 0.4647, 0.4670, 0.4645, 0.4651, 0.4654,\n",
      "        0.4664, 0.4642, 0.4654, 0.4650, 0.4657, 0.4579, 0.4647, 0.4645, 0.4647,\n",
      "        0.4646, 0.4631, 0.4649, 0.4625, 0.4644, 0.4672, 0.4624, 0.4646, 0.4652,\n",
      "        0.4649, 0.4646, 0.4356, 0.4648, 0.4640, 0.4652, 0.4854, 0.4646, 0.4640,\n",
      "        0.4651, 0.4656, 0.4644, 0.4445, 0.4647, 0.4650, 0.4645, 0.4651, 0.4669,\n",
      "        0.4647, 0.4680, 0.4647, 0.4646, 0.4644, 0.4590, 0.4616, 0.4644, 0.4674,\n",
      "        0.4649, 0.4648, 0.4648, 0.4646, 0.4649, 0.4582, 0.4639, 0.4651, 0.4638,\n",
      "        0.4646, 0.4654, 0.4646, 0.4767, 0.4641, 0.4603, 0.4652, 0.4644, 0.4660,\n",
      "        0.4679, 0.4645, 0.4645, 0.4746, 0.4644, 0.4670, 0.4644, 0.4649, 0.4644,\n",
      "        0.4642, 0.4671, 0.4650, 0.4652, 0.4732, 0.4648, 0.4652, 0.4649, 0.4642,\n",
      "        0.4646, 0.4654, 0.4643, 0.4646, 0.4659, 0.4645, 0.4648, 0.4647, 0.4581,\n",
      "        0.4542, 0.4646, 0.4652, 0.4630, 0.4616, 0.4654, 0.4650, 0.4647, 0.4659,\n",
      "        0.4646, 0.4645, 0.4649, 0.4650, 0.4648, 0.4524, 0.4638, 0.4668, 0.4649,\n",
      "        0.4768, 0.4644, 0.4649, 0.4646, 0.4636, 0.4659, 0.4649, 0.4499, 0.4651,\n",
      "        0.4648, 0.4649, 0.4720, 0.4646, 0.4647, 0.4645, 0.4655, 0.4645, 0.4653,\n",
      "        0.4641, 0.4674, 0.4647, 0.4646, 0.4645, 0.4640, 0.4646, 0.4657, 0.4650,\n",
      "        0.4515, 0.4642, 0.4749, 0.4658, 0.4648, 0.4656, 0.4638, 0.4648, 0.4651,\n",
      "        0.4660, 0.4646, 0.4656])\n",
      "Time: 1658.6573441028595\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "d=[]\n",
    "d1=[]\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net_R(images)\n",
    "        #_, predicted = torch.max(outputs.data, 1)\n",
    "        #print(predicted,predicted.shape)\n",
    "        outputs=outputs.view(outputs.size(0),-1)\n",
    "        d.append(outputs)\n",
    "        d1.append(labels)\n",
    "\n",
    "        \n",
    "\n",
    "#print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "#d=np.asarray(d)\n",
    "\n",
    "#print(d[0])\n",
    "d=d[0]\n",
    "d=d.reshape(1,-1)\n",
    "print(d1[0])\n",
    "print(d[0])\n",
    "y_pred=d[0]\n",
    "y_test=d1[0]\n",
    "\n",
    "end=time.time()\n",
    "df=end-start\n",
    "print('Time:',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import metrics\n",
    "#print(y_pred,y_test)\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y_pred, y_test)\n",
    "#plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
